{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abdbd7dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e139f9f",
   "metadata": {},
   "source": [
    "# Loading hubble measurement results\n",
    "\n",
    "Let's re-use the cell from last week where we loaded the hubble measurments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "437579ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.loadtxt(open(\"../data/Hubble.txt\", 'rb'), usecols=[1,2,3])\n",
    "# This is how we pull out the data from columns in the array.\n",
    "H0_measured = data[:,0]\n",
    "H0_errorLow = data[:,1]\n",
    "H0_errorHigh = data[:,2]\n",
    "N_measurements = H0_measured.size"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d911cd4a",
   "metadata": {},
   "source": [
    "#### Histogramming the results\n",
    "\n",
    "Here is a histogram of the results.  Note that we have expanded the x-axis.  You will see why in a few cells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "378f84fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = plt.hist(H0_measured, bins=np.linspace(55.5, 90.5, 45))\n",
    "_ = plt.xlabel(\"Hubble Constant [km/s/Mpc]\")\n",
    "_ = plt.ylabel(\"Counts [per 1.0 km/s/Mpc]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50dfa188",
   "metadata": {},
   "source": [
    "### Adding measurement errors.\n",
    "\n",
    "Now let's add the measurement errors to the plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d15784b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = plt.errorbar(H0_measured, np.arange(N_measurements), xerr=(H0_errorLow, H0_errorHigh), fmt=\".\")\n",
    "_ = plt.xlabel(\"Hubble Constant [km/s/Mpc]\")\n",
    "_ = plt.ylabel(\"Experiment number\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc1e344a",
   "metadata": {},
   "source": [
    "# Questions for discussion\n",
    "\n",
    "1.1 What do we learn from the figure with the error bars included as opposed the histogram?  Does it change your estimate of what the true value of the Hubble parameter is?  \n",
    "\n",
    "1.2 There is no single true value that is consistent with all (or even most) of the measured values.  What do you think that means?  \n",
    "\n",
    "1.3 Describe a way that you might derive an estimate of the true value of the Hubble parameter (and a uncertainty on that value) that uses both the measured values and their stated uncertaities.  \n",
    "\n",
    "Bonus Exercise, implement your method and post the code and the result (include a plot if you like) into your report."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90775627",
   "metadata": {},
   "source": [
    "## Review from last week.   Quantifying the scatter of data.\n",
    "\n",
    "Recall that last week we explored a number of different ways to quantiy the scatter in data.\n",
    "\n",
    "The last two that we discussed were the \"standard deviation\" and \"standard error\".\n",
    "\n",
    "The \"standard deviation\" is also know as the RMS (root-mean-square) of the data, and it tells use how much intrisict scatter there is in the data.\n",
    "\n",
    "The \"standard error\" accounts for the fact that if we keep measuring something, and the measurements keep falling within the same range, that actually gives us a better estimate of what the true value is, because we are averaging out the statistical fluctuations.\n",
    "\n",
    "The formula for these are:\n",
    "\n",
    "average:            $ \\mu = \\frac{\\sum_i x_i}{n}$\n",
    "\n",
    "standard deviation: $ \\sigma = (\\frac{\\sum_i (x_i - \\mu)^2}{n})^{1/2} $\n",
    "\n",
    "standard error:     $ \\frac{\\sigma}{\\sqrt{n}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "791dc6f4",
   "metadata": {},
   "source": [
    "## The \"variance\", an extermely useful quantity.  \n",
    "\n",
    "The \"variance\" of a distribution is the square of the standard deviation, and it has a number of useful properties that we will explore later in this course.  For now I'm just going to write down the equation.\n",
    "\n",
    "variance : $\\sigma^2 = \\frac{\\sum_i (x_i - \\mu)^2}{n}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a7377ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "variance = np.var(H0_measured)\n",
    "variance_check = np.std(H0_measured)**2\n",
    "print(\"Variance:       \", variance)\n",
    "print(\"Variance check: \", variance_check)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65fd8a5a",
   "metadata": {},
   "source": [
    "# Simulating data from a bell curve \n",
    "\n",
    "## (a.k.a. a \"Gaussian\" or \"Normal\" distribution)\n",
    "\n",
    "We will be discussing the \"Gaussian\" or \"Normal\" distribution a lot more in coming weeks.  \n",
    "\n",
    "For now let's just state that in many cases a Normal distribution is a good representation the sort of random variations you expect to see if data if you perform a measurement many times.\n",
    "\n",
    "The function 'np.random.normal' will generate values from a Gaussian distribtuion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2def042",
   "metadata": {},
   "outputs": [],
   "source": [
    "#help(np.random.normal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4ac763e",
   "metadata": {},
   "outputs": [],
   "source": [
    "N_measurements = len(H0_measured)\n",
    "H0_mean = np.mean(H0_measured)\n",
    "H0_std = np.std(H0_measured)\n",
    "H0_err = H0_std / np.sqrt(N_measurements)\n",
    "print(\"H0 = %0.3f +- %0.3f\" % (H0_mean, H0_err))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8596447",
   "metadata": {},
   "outputs": [],
   "source": [
    "simData = np.random.normal(loc=H0_mean, scale=H0_std, size=100000)\n",
    "_ = plt.hist(simData, bins=np.linspace(55., 85., 121))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c640040b",
   "metadata": {},
   "source": [
    "### Simulating a bunch of measurements of the Hubble constant\n",
    "\n",
    "Now we are going to pretend that we sent out 10 teams of scientist, and asked each of them to do 10 measurements of the Hubble constant, and that all the measurments are draw from the distribution above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "509e1b1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataSample_0 = np.random.normal(loc=H0_mean, scale=H0_std, size=10)\n",
    "dataSample_1 = np.random.normal(loc=H0_mean, scale=H0_std, size=10)\n",
    "dataSample_2 = np.random.normal(loc=H0_mean, scale=H0_std, size=10)\n",
    "dataSample_3 = np.random.normal(loc=H0_mean, scale=H0_std, size=10)\n",
    "dataSample_4 = np.random.normal(loc=H0_mean, scale=H0_std, size=10)\n",
    "dataSample_5 = np.random.normal(loc=H0_mean, scale=H0_std, size=10)\n",
    "dataSample_6 = np.random.normal(loc=H0_mean, scale=H0_std, size=10)\n",
    "dataSample_7 = np.random.normal(loc=H0_mean, scale=H0_std, size=10)\n",
    "dataSample_8 = np.random.normal(loc=H0_mean, scale=H0_std, size=10)\n",
    "dataSample_9 = np.random.normal(loc=H0_mean, scale=H0_std, size=10)\n",
    "dataSamples = [dataSample_0, dataSample_1, dataSample_2, dataSample_3, dataSample_4,\n",
    "               dataSample_5, dataSample_6, dataSample_7, dataSample_8, dataSample_9]\n",
    "mergedSample = np.vstack(dataSamples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "046c0f47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This cell is just so that you can play around and understand what the function np.vstack did\n",
    "print(mergedSample.shape)\n",
    "print(dataSample_0)\n",
    "print(mergedSample[0,:])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b92b237c",
   "metadata": {},
   "source": [
    "### Now we are going to use numpy to do some statistics for us"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15c2b766",
   "metadata": {},
   "outputs": [],
   "source": [
    "means = np.mean(mergedSample, axis=1)\n",
    "stds = np.std(mergedSample, axis=1)\n",
    "errors = stds/np.sqrt(10)\n",
    "for mean, err in zip(means, errors):\n",
    "    print(\"Value: %0.2f +- %0.2f\" % (mean, err))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae5faef8",
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = plt.errorbar(means, np.arange(10), xerr=(errors), fmt=\".\")\n",
    "_ = plt.xlim(68.,75.)\n",
    "_ = plt.xlabel(\"Mean of sub-sample\")\n",
    "_ = plt.ylabel(\"Experiment number\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fd5be05",
   "metadata": {},
   "outputs": [],
   "source": [
    "overall_mean = np.mean(mergedSample)\n",
    "overall_std = np.std(mergedSample)\n",
    "overall_error = np.std(mergedSample) / np.sqrt(100)\n",
    "print(\"Mean:  %0.3f +- %0.3f\" % (overall_mean, overall_error))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9555f86d",
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = 1./(errors*errors)\n",
    "weighted_mean = np.sum(means*weights)/np.sum(weights)\n",
    "weighted_error = np.sqrt(1/np.sum(weights))\n",
    "print(\"Weighted mean: %0.3f +- %0.3f\" % (weighted_mean, weighted_error))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc2fe118",
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = plt.errorbar(means, np.arange(10), xerr=(errors), fmt=\".\")\n",
    "_ = plt.xlim(68., 75)\n",
    "_ = plt.xlabel(\"Mean of sub-sample\")\n",
    "_ = plt.ylabel(\"Experiment number\")\n",
    "_ = plt.errorbar(overall_mean, 4.6, xerr=overall_error, yerr=5, fmt='o')\n",
    "_ = plt.errorbar(weighted_mean, 4.4, xerr=weighted_error, yerr=5, fmt='o')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7950a0a",
   "metadata": {},
   "source": [
    "### Now lets try the same thing with the hubble measurements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32bb8ec7",
   "metadata": {},
   "outputs": [],
   "source": [
    "H0_errors = H0_errorLow + H0_errorHigh\n",
    "H0_weights = 1./(H0_errors*H0_errors)\n",
    "H0_wmean = np.sum(H0_measured*H0_weights)/np.sum(H0_weights)\n",
    "H0_werror = np.sqrt(1/np.sum(H0_weights))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a254e82b",
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = plt.errorbar(H0_measured, np.arange(N_measurements), xerr=(H0_errorLow, H0_errorHigh), fmt=\".\")\n",
    "_ = plt.xlabel(\"Hubble Constant [km/s/Mpc]\")\n",
    "_ = plt.ylabel(\"Experiment number\")\n",
    "_ = plt.errorbar(H0_wmean, 18, xerr=H0_werror, yerr=20, fmt='X')\n",
    "_ = plt.errorbar(H0_mean, 18, xerr=H0_err, yerr=20, fmt='o')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fafc9ed",
   "metadata": {},
   "source": [
    "# Questions for discussion:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ec7c2fe",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
