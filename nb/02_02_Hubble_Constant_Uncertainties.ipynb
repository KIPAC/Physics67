{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abdbd7dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa13e6f6",
   "metadata": {},
   "source": [
    "### New functions we will use in this module\n",
    "\n",
    "| Function Name            | What it does |\n",
    "| - | - |\n",
    "| numpy.var                | Compute the variance of the values in an array |\n",
    "| numpy.random.normal      | Generate random numbers from a normal or 'Gaussian' distribution |\n",
    "| array.size               | return the number of elements in an array |\n",
    "| array.shape              | return the shape of an array, i.e., arrays have more that one dimension and this function tells you the shape of the array.  The size of the array is the product of the size of all the axes of the array |\n",
    "| plt.errorbar             | make a plot with error bars |\n",
    "| plt.legend               | add a legend to a figure |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e139f9f",
   "metadata": {},
   "source": [
    "# Loading hubble measurement results\n",
    "\n",
    "Let's re-use the cell from last week where we loaded the hubble measurments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "437579ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.loadtxt(open(\"../data/Hubble.txt\", 'rb'), usecols=[1,2,3])\n",
    "# If for some reason that doesn't work, then you can download that file to your jupyter area and do:\n",
    "# data = np.loadtxt(open(\"Hubble.txt\", 'rb'), usecols=[1,2,3])\n",
    "# This is how we pull out the data from columns in the array.\n",
    "H0_measured = data[:,0]\n",
    "H0_errorLow = data[:,1]\n",
    "H0_errorHigh = data[:,2]\n",
    "N_measurements = H0_measured.size"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d911cd4a",
   "metadata": {},
   "source": [
    "#### Histogramming the results\n",
    "\n",
    "Here is a histogram of the results.  Note that we have expanded the x-axis.  You will see why in a few cells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "378f84fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = plt.hist(H0_measured, bins=np.linspace(55.5, 90.5, 45))\n",
    "_ = plt.xlabel(\"Hubble Constant [km/s/Mpc]\")\n",
    "_ = plt.ylabel(\"Counts [per 1.0 km/s/Mpc]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50dfa188",
   "metadata": {},
   "source": [
    "### Adding measurement errors.\n",
    "\n",
    "Now let's add the measurement errors to the plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d15784b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = plt.errorbar(H0_measured, np.arange(N_measurements), xerr=(H0_errorLow, H0_errorHigh), fmt=\".\")\n",
    "_ = plt.xlabel(\"Hubble Constant [km/s/Mpc]\")\n",
    "_ = plt.ylabel(\"Experiment number\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc1e344a",
   "metadata": {},
   "source": [
    "# Questions for discussion\n",
    "\n",
    "4.1 What do we learn from the figure with the error bars included as opposed to the histogram?  Does it change your estimate of what the true value of the Hubble parameter is?  \n",
    "\n",
    "4.2 There is no single true value that is consistent with all (or even most) of the measured values.  What do you think that means?  \n",
    "\n",
    "4.3 Before proceeding, describe a way that you might derive an estimate of the true value of the Hubble parameter (and an uncertainty on that value) that uses both the measured values and their stated uncertainties.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90775627",
   "metadata": {},
   "source": [
    "## Review from last week.   Quantifying the scatter of data.\n",
    "\n",
    "Recall that last week we explored a number of different ways to quantiy the scatter in data.\n",
    "\n",
    "The last two that we discussed were the \"standard deviation\" and \"standard error\".\n",
    "\n",
    "The \"standard deviation\" is also know as the RMS (root-mean-square) of the data, and it tells use how much intrisict scatter there is in the data.\n",
    "\n",
    "The \"standard error\" accounts for the fact that if we keep measuring something, and the measurements keep falling within the same range, that actually gives us a better estimate of what the true value is, because we are averaging out the statistical fluctuations.\n",
    "\n",
    "The formula for these are:\n",
    "\n",
    "average:            $ \\mu = \\frac{\\sum_i x_i}{n}$\n",
    "\n",
    "standard deviation: $ \\sigma = (\\frac{\\sum_i (x_i - \\mu)^2}{n})^{1/2} $\n",
    "\n",
    "standard error:     $ \\frac{\\sigma}{\\sqrt{n}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "791dc6f4",
   "metadata": {},
   "source": [
    "## The \"variance\", an extermely useful quantity.  \n",
    "\n",
    "The \"variance\" of a distribution is the square of the standard deviation, and it has a number of useful properties that we will explore later in this course.  For now I'm just going to write down the equation.\n",
    "\n",
    "variance : $\\sigma^2 = \\frac{\\sum_i (x_i - \\mu)^2}{n}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a7377ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "variance = np.var(H0_measured)\n",
    "variance_check = np.std(H0_measured)**2\n",
    "print(\"Variance:       \", variance)\n",
    "print(\"Variance check: \", variance_check)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65fd8a5a",
   "metadata": {},
   "source": [
    "# Simulating data from a bell curve \n",
    "\n",
    "## (a.k.a. a \"Gaussian\" or \"Normal\" distribution)\n",
    "\n",
    "We will be discussing the \"Gaussian\" or \"Normal\" distribution a lot more in coming weeks.  \n",
    "\n",
    "For now let's just state that in many cases a Normal distribution is a good representation the sort of random variations you expect to see if data if you perform a measurement many times.\n",
    "\n",
    "The function 'np.random.normal' will generate values from a Gaussian distribtuion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2def042",
   "metadata": {},
   "outputs": [],
   "source": [
    "#help(np.random.normal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4ac763e",
   "metadata": {},
   "outputs": [],
   "source": [
    "N_measurements = len(H0_measured)\n",
    "H0_mean = np.mean(H0_measured)\n",
    "H0_std = np.std(H0_measured)\n",
    "H0_err = H0_std / np.sqrt(N_measurements)\n",
    "print(\"H0 = %0.3f +- %0.3f\" % (H0_mean, H0_err))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8596447",
   "metadata": {},
   "outputs": [],
   "source": [
    "simData = np.random.normal(loc=H0_mean, scale=H0_std, size=100000)\n",
    "_ = plt.hist(simData, bins=np.linspace(55., 85., 121))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c640040b",
   "metadata": {},
   "source": [
    "### Simulating a bunch of measurements of the Hubble constant\n",
    "\n",
    "Now we are going to pretend that we sent out 10 teams of scientist, and asked each of them to do some measurements of the Hubble constant, and that all the measurments are draw from the distribution above.   The groups of scientiest do a different number of measurements, but in total they have 100 measurements.\n",
    "\n",
    "We are then going to consider two different ways of combining their results.\n",
    "\n",
    "1. Taking the overall mean and error on the mean from all 100 measurements.\n",
    "2. Taking the mean and the error on the mean from each group of scientists, and making a weighted average.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "509e1b1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataSample_0 = np.random.normal(loc=H0_mean, scale=H0_std, size=20)\n",
    "dataSample_1 = np.random.normal(loc=H0_mean, scale=H0_std, size=4)\n",
    "dataSample_2 = np.random.normal(loc=H0_mean, scale=H0_std, size=12)\n",
    "dataSample_3 = np.random.normal(loc=H0_mean, scale=H0_std, size=10)\n",
    "dataSample_4 = np.random.normal(loc=H0_mean, scale=H0_std, size=16)\n",
    "dataSample_5 = np.random.normal(loc=H0_mean, scale=H0_std, size=7)\n",
    "dataSample_6 = np.random.normal(loc=H0_mean, scale=H0_std, size=3)\n",
    "dataSample_7 = np.random.normal(loc=H0_mean, scale=H0_std, size=8)\n",
    "dataSample_8 = np.random.normal(loc=H0_mean, scale=H0_std, size=11)\n",
    "dataSample_9 = np.random.normal(loc=H0_mean, scale=H0_std, size=9)\n",
    "dataSamples = [dataSample_0, dataSample_1, dataSample_2, dataSample_3, dataSample_4,\n",
    "               dataSample_5, dataSample_6, dataSample_7, dataSample_8, dataSample_9]\n",
    "mergedSample = np.hstack(dataSamples)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b92b237c",
   "metadata": {},
   "source": [
    "### Now we are going to use numpy to do some statistics for us\n",
    "\n",
    "Pro-tip: python gives you a shortcut for writing out loops\n",
    "\n",
    "for example `np.array([np.mean(dataSample) for dataSample in dataSamples])` tells python \n",
    "\n",
    "1. loop over all the elements in the list of dataSamples\n",
    "2. for each element to take the mean\n",
    "3. add that mean onto a list\n",
    "\n",
    "This construction is called \"list comprehension\" and is used a lot by python programmers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15c2b766",
   "metadata": {},
   "outputs": [],
   "source": [
    "means = np.array([np.mean(dataSample) for dataSample in dataSamples])\n",
    "stds = np.array([np.std(dataSample) for dataSample in dataSamples])\n",
    "errors = np.array([np.std(dataSample)/np.sqrt(len(dataSample)) for dataSample in dataSamples])\n",
    "for mean, err in zip(means, errors):\n",
    "    print(\"Value: %0.2f +- %0.2f\" % (mean, err))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1fd4d92",
   "metadata": {},
   "source": [
    "### Now lets plot the summary results that each group of scientists might report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae5faef8",
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = plt.errorbar(means, np.arange(10), xerr=(errors), fmt=\".\")\n",
    "_ = plt.xlim(68.,75.)\n",
    "_ = plt.xlabel(\"Mean of sub-sample\")\n",
    "_ = plt.ylabel(\"Experiment number\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7260cc33",
   "metadata": {},
   "source": [
    "### Questions for discussion, \n",
    "\n",
    "5.1 Even though all of the measurements are being simulated from the same distribution, the different groups of scientist are reporting different uncertainties for their best estimates.  Why is that?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36444bf0",
   "metadata": {},
   "source": [
    "###  Now we compute the two different estimates of the true value and the uncertainty on that value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fd5be05",
   "metadata": {},
   "outputs": [],
   "source": [
    "overall_mean = np.mean(mergedSample)\n",
    "overall_std = np.std(mergedSample)\n",
    "overall_error = np.std(mergedSample) / np.sqrt(100)\n",
    "print(\"Mean:  %0.3f +- %0.3f\" % (overall_mean, overall_error))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9555f86d",
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = 1./(errors*errors)\n",
    "weighted_mean = np.sum(means*weights)/np.sum(weights)\n",
    "weighted_error = np.sqrt(1/np.sum(weights))\n",
    "straight_mean = np.mean(means)\n",
    "straight_error = np.std(means)/np.sqrt(10)\n",
    "print(\"Straight mean: %0.3f +- %0.3f\" % (straight_mean, straight_error))\n",
    "print(\"Weighted mean: %0.3f +- %0.3f\" % (weighted_mean, weighted_error))\n",
    "print(\"Truth        : %0.3f\" % (H0_mean))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39f59239",
   "metadata": {},
   "source": [
    "###  Now lets add those to our plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc2fe118",
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = plt.errorbar(means, np.arange(10), xerr=(errors), fmt=\".\", color='k')\n",
    "_ = plt.xlim(68., 75)\n",
    "_ = plt.xlabel(\"Mean of sub-sample\")\n",
    "_ = plt.ylabel(\"Experiment number\")\n",
    "_ = plt.errorbar(overall_mean, 4.6, xerr=overall_error, yerr=5, fmt='o', color='g', label=\"Full Sample\")\n",
    "_ = plt.errorbar(straight_mean, 4.2, xerr=straight_error, yerr=5, fmt='o', color='r', label=\"Mean\")\n",
    "_ = plt.errorbar(weighted_mean, 4.4, xerr=weighted_error, yerr=5, fmt='o', color='b', label=\"Weighted Mean\")\n",
    "_ = plt.scatter(H0_mean, 4.8, marker='o', color='cyan', label=\"True\")\n",
    "_ = plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e43d936",
   "metadata": {},
   "source": [
    "# Questions for discussion:\n",
    "\n",
    "5.1 In your own words, describe what is being presented in the previous plot and the two cells before that.\n",
    "\n",
    "5.2 In this case, the different methods of combining the results give quite similar, but not identical, final answers.  \n",
    "\n",
    "5.3 The usual convention for combining results is to use variance weighting (i.e., the weighted mean as computed here).  Does that seem like a sensible convention to you?  Why or why not?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7950a0a",
   "metadata": {},
   "source": [
    "### Now lets try the using the uncertainties to get a weighted average of the Hubble measurements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32bb8ec7",
   "metadata": {},
   "outputs": [],
   "source": [
    "H0_errors = H0_errorLow + H0_errorHigh\n",
    "H0_weights = 1./(H0_errors*H0_errors)\n",
    "H0_wmean = np.sum(H0_measured*H0_weights)/np.sum(H0_weights)\n",
    "H0_werror = np.sqrt(1/np.sum(H0_weights))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a254e82b",
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = plt.errorbar(H0_measured, np.arange(N_measurements), xerr=(H0_errorLow, H0_errorHigh), fmt=\".\", color='k')\n",
    "_ = plt.xlabel(\"Hubble Constant [km/s/Mpc]\")\n",
    "_ = plt.ylabel(\"Experiment number\")\n",
    "_ = plt.errorbar(H0_mean, 18, xerr=H0_werror, yerr=20, fmt='o', color='b', label=\"Mean\")\n",
    "_ = plt.errorbar(H0_wmean, 18, xerr=H0_err, yerr=20, fmt='o', color=\"r\", label=\"Weighted Mean\")\n",
    "_ = plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fafc9ed",
   "metadata": {},
   "source": [
    "# Questions for discussion:\n",
    "\n",
    "6.1 In this case the weighted mean doesn't quite agree with the straight mean.  Looking at the plot, why do you think that is?\n",
    "\n",
    "6.2 Does this change what you think about that the best estimate of the Hubble parameter (and it's uncertainty) might be?  Why?  What about the uncertainty on the Hubble parameter?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ec7c2fe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce94aea3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
