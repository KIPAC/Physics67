{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebfed2f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import scipy.stats as stats\n",
    "import scipy.optimize as optimize\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9561c128",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.loadtxt(open(\"../data/Vela_Flux.txt\", 'rb'), usecols=range(7))\n",
    "\n",
    "# This is how we pull out the data from columns in the array.\n",
    "\n",
    "# This is the date in \"Mission Elapsesd Time\"\n",
    "# For the Fermi mission, this is defined to be the number of seconds since the start of 2001.\n",
    "date_MET = data[:,0]\n",
    "# This is the offset in seconds between the Fermi \"MET\" and the UNIX \"epoch\" used by matplotlib\n",
    "MET_To_Unix = 978336000\n",
    "\n",
    "# These are the numbers of photons observed from Vela each week in the \"low\" Energy Band (100 MeV - 800 MeV)\n",
    "nObs_LE = data[:,1]\n",
    "\n",
    "# These are the number of photons expected from Vela each week, under the assumption that it is \n",
    "# not varying at all, and the only differences depend on how long we spent looking at Vela\n",
    "# that particular weeek\n",
    "nExp_LE = data[:,2]\n",
    "\n",
    "# These are the band bounds, in MeV\n",
    "LE_bounds = (100., 800.)\n",
    "\n",
    "# This is the \"significance\" of the variation for each week.  We will discuss this more later\n",
    "signif_LE = data[:,3]\n",
    "\n",
    "nObs_HE = data[:,4]\n",
    "nExp_HE = data[:,5]\n",
    "signif_HE = data[:6]\n",
    "HE_bounds = (800., 10000.)\n",
    "\n",
    "# This converts the dates to something that matplotlib understands\n",
    "dates = [datetime.datetime.fromtimestamp(date + MET_To_Unix) for date in date_MET]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eecd0065",
   "metadata": {},
   "outputs": [],
   "source": [
    "excess_counts = nObs_LE-nExp_LE\n",
    "_ = plt.scatter(dates, excess_counts)\n",
    "_ = plt.xlabel(r\"Date [year]]\")\n",
    "_ = plt.ylabel(r\"$n_{\\rm obs}$ [per week]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85f7838f",
   "metadata": {},
   "outputs": [],
   "source": [
    "date_YEAR = 2001 +  (date_MET / (24*3600*365))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0fa08d6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7284420b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"The mean and standard deviation of the observation time are %0.2f, %0.2f\" \n",
    "      % (np.mean(date_YEAR), np.std(date_YEAR)))\n",
    "print(\"The variance is the sqaure of the standard deviation: %0.2f\" % np.var(date_YEAR))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aff4dafe",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"The mean and standard deviation of the excess counts are %0.2f, %0.2f\" \n",
    "      % (np.mean(excess_counts), np.std(excess_counts)))\n",
    "print(\"The variance is the sqaure of the standard deviation: %0.1f\" % np.var(excess_counts))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2b62921",
   "metadata": {},
   "source": [
    "### Variances and covariances:\n",
    "\n",
    "The variance is a measure of the scatter of a quantity.\n",
    "\n",
    "$\\sigma^2 = \\frac{\\sum_i (x_i - \\mu_x)}{n}^2$\n",
    "\n",
    "Where $\\mu_x$ is the mean of the measurements $\\mu_x = \\frac{\\sum_i x_i}{n}$\n",
    "\n",
    "The covariance is a measure of variantions in one quantity match variations in a second quantity.  \n",
    "\n",
    "The equation for the covariance is quite similar to the equation for the variance:\n",
    "\n",
    "$\\sigma_{xy} = \\frac{\\sum_i (x_i - \\mu_x) (y_i - \\mu_y)}{n}$\n",
    "\n",
    "I.e., we replace one of the factors of $(x_i - \\mu_x)$ with $(y_i - \\mu_y)$\n",
    "\n",
    "Because the equations are so similar, we often compute both the variances and covariances at the same time, and we often pack both the variances and covariance into a single matrix.\n",
    "\n",
    "In our case we have two quantities: the dates and the excess counts.  We have already computed the variance of each of those.  So let's compute it all:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c68bccf",
   "metadata": {},
   "outputs": [],
   "source": [
    "cov = np.cov(date_YEAR, excess_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67e574fb",
   "metadata": {},
   "source": [
    "### Let's have a look at the pieces of the covariance matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "094e1c03",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"The xx element of the covarience matrix is %.2f\" %  cov[0,0])\n",
    "print(\"The scatter of the x element (i.e., the year) is %.2f years\" % np.sqrt(cov[0,0]))\n",
    "print(\"The yy element of the covarience matrix is %.2f\" %  cov[1,1])\n",
    "print(\"The scatter of the yy element (i.e., the excess counts) is %.1f counts\" % np.sqrt(cov[1,1]))\n",
    "print(\"The xy element of the covarience matrix is %.2f years*counts\" %  cov[0,1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e5a4d69",
   "metadata": {},
   "source": [
    "### Another way to consider correlations is to ask what part of the variance in one quantity is  tied to the variance of another quanity.\n",
    "\n",
    "To do this, we want to factor out the variances of the two quantities.\n",
    "\n",
    "$c_{xy} = \\frac{\\sigma_{xy}}{\\sqrt{\\sigma_{xx}\\sigma_{yy}}}$\n",
    "\n",
    "Note that $c_{xx} = c_{yy} = 1$, i.e., each quantity is 100% correlated with itself.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44d412a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.corrcoef(date_MET, nObs_LE-nExp_LE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66110f5a",
   "metadata": {},
   "source": [
    "### Interpretation\n",
    "\n",
    "What this is saying is that about 1% of the variance in the excess is attributable to the change in time."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4586548e",
   "metadata": {},
   "source": [
    "### comparision with quantities that are highly correlated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01b20261",
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = plt.scatter(nExp_LE, nObs_LE)\n",
    "_ = plt.xlabel(r\"$n_{\\rm exp}$ [per week]\")\n",
    "_ = plt.ylabel(r\"$n_{\\rm obs}$ [per week]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12b158bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.corrcoef(nObs_LE, nExp_LE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10c16e78",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_2d_gaussian(n, sigma_xx, sigma_yy, sigma_xy):\n",
    "    \n",
    "    K_0 = np.array([[sigma_xx, sigma_xy],[sigma_xy, sigma_yy]])\n",
    "    epsilon = 0.0001\n",
    "    K = K_0 + epsilon*np.identity(2)\n",
    "    L = np.linalg.cholesky(K)\n",
    "    u = np.random.normal(size=2*n).reshape(2, n)\n",
    "    x = np.dot(L, u)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b052f101",
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_2d_gaussian(n, sigma_xx, sigma_yy, sigma_xy):\n",
    "    vals = gen_2d_gaussian(n, sigma_xx, sigma_yy, sigma_xy)\n",
    "    _ = plt.xlim(-5, 5)\n",
    "    _ = plt.ylim(-5, 5)\n",
    "    _ = plt.scatter(vals[0], vals[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16e90606",
   "metadata": {},
   "outputs": [],
   "source": [
    "vals = draw_2d_gaussian(1000, 1, 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6365f4f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "vals = draw_2d_gaussian(1000, 1, 1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfcc6919",
   "metadata": {},
   "outputs": [],
   "source": [
    "vals = draw_2d_gaussian(1000, 1, 1, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e1beeb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "vals = draw_2d_gaussian(1000, 1, 1, 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7759e1ee",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
