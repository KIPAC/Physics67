{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd6bc82e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import scipy.stats as stats\n",
    "import scipy.optimize as optimize\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8531c449",
   "metadata": {},
   "source": [
    "### New functions we will use in this module\n",
    "\n",
    "| Function Name            | What it does |\n",
    "| - | - |\n",
    "|  plt.contour |  Make a contour plot, ie., show the contours correspond to a series of values |\n",
    "| scipy.stats.chi2 | Interact with a $\\chi^2$ distribution, e.g., to compute a p-value |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2df2c8f",
   "metadata": {},
   "source": [
    "### Ok let's pick up where we left off with the Vela pulsar data\n",
    "\n",
    "(This cell is just a repeat of loading the Vela data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80b70a73",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.loadtxt(open(\"../data/Vela_Flux.txt\", 'rb'), usecols=range(7))\n",
    "\n",
    "# This is how we pull out the data from columns in the array.\n",
    "\n",
    "# This is the date in \"Mission Elapsesd Time\"\n",
    "# For the Fermi mission, this is defined to be the number of seconds since the start of 2001.\n",
    "date_MET = data[:,0]\n",
    "# This is the offset in seconds between the Fermi \"MET\" and the UNIX \"epoch\" used by matplotlib\n",
    "MET_To_Unix = 978336000\n",
    "\n",
    "# These are the numbers of photons observed from Vela each week in the \"low\" Energy Band (100 MeV - 800 MeV)\n",
    "nObs_LE = data[:,1]\n",
    "\n",
    "# These are the number of photons expected from Vela each week, under the assumption that it is \n",
    "# not varying at all, and the only differences depend on how long we spent looking at Vela\n",
    "# that particular weeek\n",
    "nExp_LE = data[:,2]\n",
    "\n",
    "# These are the band bounds, in MeV\n",
    "LE_bounds = (100., 800.)\n",
    "\n",
    "# This is the \"significance\" of the variation for each week.  We will discuss this more later\n",
    "signif_LE = data[:,3]\n",
    "\n",
    "nObs_HE = data[:,4]\n",
    "nExp_HE = data[:,5]\n",
    "signif_HE = data[:6]\n",
    "HE_bounds = (800., 10000.)\n",
    "\n",
    "# This converts the dates to something that matplotlib understands\n",
    "dates = [datetime.datetime.fromtimestamp(date + MET_To_Unix) for date in date_MET]\n",
    "date_YEAR = 2001 +  (date_MET / (24*3600*365))\n",
    "years_since_mid_2014 = date_YEAR  - 2014.5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36d0a1cc",
   "metadata": {},
   "source": [
    "So, we are going to investigate if the small correlation between the excess and the time is statistically significant.   Let's start by looking at the data and adding error bars to the points and plotting it using the variables we just defined."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a7a093d",
   "metadata": {},
   "outputs": [],
   "source": [
    "excess_counts = nObs_LE-nExp_LE\n",
    "sigma_counts =  np.sqrt(nObs_LE)\n",
    "_ = plt.errorbar(years_since_mid_2014, excess_counts, yerr=sigma_counts)\n",
    "_ = plt.xlabel(r\"Date w.r.t mid 2014 [years]\")\n",
    "_ = plt.ylabel(r\"$n_{\\rm obs}$ [per week]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8d696e5",
   "metadata": {},
   "source": [
    "# Step 1.   The model\n",
    "\n",
    "We just write a simple function for our model.   And plot how the model looks for a few values of the parameters.\n",
    "\n",
    "$m_{\\rm ex}(t | p_0, p_1) = p_0 + t * p_1$ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7b798ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_function(xvals, params):\n",
    "    return params[0] + xvals*params[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffa1cb89",
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = plt.xlabel(r\"Time since mid 2014 [years]\")\n",
    "_ = plt.ylabel(r\"$n_{\\rm ex}$ [per week]\")\n",
    "_ = plt.errorbar(years_since_mid_2014, excess_counts, yerr=sigma_counts, alpha=0.2)\n",
    "\n",
    "xvals = years_since_mid_2014\n",
    "params = np.array([0, 0])\n",
    "for offset in np.linspace(-200, 200, 5):\n",
    "    params[0] = offset\n",
    "    _ = plt.plot(xvals, linear_function(xvals, params), label=r\"Offset = %0.0f\" % offset)\n",
    "_ = plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf2d4777",
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = plt.xlabel(r\"Time since mid 2014 [years]\")\n",
    "_ = plt.ylabel(r\"$n_{\\rm ex}$ [per week]\")\n",
    "_ = plt.errorbar(years_since_mid_2014, excess_counts, yerr=sigma_counts, alpha=0.2)\n",
    "\n",
    "xvals = years_since_mid_2014\n",
    "params = np.array([0, 0])\n",
    "for slope in np.linspace(-15, 15, 5):\n",
    "    params[1] = slope\n",
    "    _ = plt.plot(xvals, linear_function(xvals, params), label=r\"Slope = %0.1f\" % slope)\n",
    "_ = plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ebbc6d3",
   "metadata": {},
   "source": [
    "# Questions for discussion\n",
    "\n",
    "4.1  What do you think about our model?  Is this a sensible model?   What would it mean if $p_0$ were very different from 0?  What about if $p_1$ were different from zero?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10a78c78",
   "metadata": {},
   "source": [
    "# Step 2.   The residuals\n",
    "\n",
    "Let's write a function to plot the residuals given the model parameters.  The indvidual residuals are: $\\delta_i = n_{\\rm ex,i} - m_{\\rm ex,i} = n_{\\rm ex,i} - (p_0  + t * p_1)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9c255a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def residual_function(data_x, data_y, model_function, params):\n",
    "    model_y = model_function(data_x, params)\n",
    "    residual = data_y - model_y\n",
    "    return residual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f6adcab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_residuals(xvals, yvals_list):\n",
    "    _ = plt.xlabel(r\"Time since mid 2014 [years]\")\n",
    "    _ = plt.ylabel(r\"$\\delta$ [counts]\")\n",
    "    for yvals in yvals_list:\n",
    "        _ = plt.scatter(xvals, yvals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a1fb1fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def hist_residuals(yvals_list):\n",
    "    _ = plt.xlabel(r\"$\\delta$ [counts]\")\n",
    "    _ = plt.ylabel(\"Weeks [per 10 counts]\")\n",
    "    for yvals in yvals_list:\n",
    "        _ = plt.hist(yvals, bins=np.linspace(-300, 300, 61))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f57872a",
   "metadata": {},
   "outputs": [],
   "source": [
    "resid_0_0 = residual_function(years_since_mid_2014, excess_counts, linear_function, [0,0])\n",
    "resid_0_20 = residual_function(years_since_mid_2014, excess_counts, linear_function, [0,20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6032bb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_residuals(years_since_mid_2014, [resid_0_20, resid_0_0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f4bc1f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "hist_residuals([resid_0_20, resid_0_0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "971c2cfe",
   "metadata": {},
   "source": [
    "# Step 3.   The scaled residuals\n",
    "\n",
    "Let's write a function to plot the scaled residuals (i.e.,  the residuals divided by the uncertainties on the data points to scale them down to a unit Gaussian) given the model parameters:\n",
    "\n",
    "$\\chi_i =  \\frac{n_{\\rm ex,i} - m_{\\rm ex,i}}{\\sigma_{obs, i}} = \\frac{n_{\\rm ex,i} - (p_0  + t_i * p_1)}{\\sigma_{obs, i}}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d828d80a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def hist_scaled_residuals(yvals_list):\n",
    "    _ = plt.xlabel(r\"$\\delta$ [$\\sigma$]\")\n",
    "    _ = plt.ylabel(r\"Weeks [per $0.2 \\sigma$]\")\n",
    "    for yvals in yvals_list:\n",
    "        _ = plt.hist(yvals, bins=np.linspace(-5, 5, 51))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0e66bf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "chi_0_0 = resid_0_0/sigma_counts\n",
    "chi_0_20 = resid_0_20/sigma_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82a7a446",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_residuals(years_since_mid_2014, [chi_0_20, chi_0_0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91086c12",
   "metadata": {},
   "outputs": [],
   "source": [
    "hist_scaled_residuals([chi_0_20, chi_0_0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23e42b45",
   "metadata": {},
   "source": [
    "#  Questions for discussion.\n",
    "\n",
    "5.1 Using what we learned last week about Gaussian distributions and p-values, explain why the two plots with the scaled residuals are much more useful than the two plots with the unscaled residuals.\n",
    "\n",
    "5.2 Given the plot above, do you think that there is any chance that the true value of $p_1$ is actually 20 counts / year?  Explain why or why not."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc4bd176",
   "metadata": {},
   "source": [
    "# Step 4.   The $\\chi^2$\n",
    "\n",
    "Let's write a function to compute the $\\chi^2$ given the model parameters:  $\\chi^2 = \\sum_i \\chi_i^2$.\n",
    "\n",
    "Note that given a number of \"degrees of freedom\", which are equal to the number of data points - the number of parameters we are fitting, for ideal data the expected distribution of $\\chi^2$ is known (and given by the scipy.stats.chi2 function)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0091785b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chi2_function(data_x, data_y, data_sigma_y, model_function, params):\n",
    "    model_y = model_function(data_x, params)\n",
    "    chi2 = ((data_y - model_y)/(data_sigma_y))**2\n",
    "    return np.sum(chi2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60186d1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "chi2_0_0 = chi2_function(years_since_mid_2014, excess_counts, sigma_counts, linear_function, [0,0])\n",
    "chi2_0_20 = chi2_function(years_since_mid_2014, excess_counts, sigma_counts, linear_function, [0,20])\n",
    "chi2_check = np.sum(chi_0_0*chi_0_0)\n",
    "print(\"chi**2(0,0)  : %0.1f\" % chi2_0_0)\n",
    "print(\"chi**2 check : %0.1f\" % chi2_check)\n",
    "print(\"chi**2(0,20) : %0.1f\" % chi2_0_20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "709b021d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"The odds of seeing the observed data given params (0.,0.) are %.1e\" % stats.chi2(df=chi_0_0.size-2).sf(chi2_0_0))\n",
    "print(\"The odds of seeing the observed data given params (0.,20.) are %.1e\" % stats.chi2(df=chi_0_20.size-2).sf(chi2_0_20))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f66e34b",
   "metadata": {},
   "source": [
    "# Question for discussion.\n",
    "\n",
    "6.1 We just saw that in an idealize case, chance of seeing the observed data given model parameters (0.,20.) is astonishingly less likely (by a factor of $10^{126}$ than the change of seeing the observed data if the model parameters were (0., 0.).  Explain why this is, using information from the plots above question 5.2."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bda8cd5",
   "metadata": {},
   "source": [
    "# Step 5.  Minimizing the $\\chi^2$\n",
    "\n",
    "Ok, first we are going to try find the set of parameters that given us the best $\\chi^2$ value.  \n",
    "\n",
    "We have constructed the $\\chi^2$ by summing together in quadrature a bunch of quantities that, in an ideal world, are each distributed as a Unit Gaussian.  This gives us a really powerful tool to interpret changes in the $\\chi^2$ value in terms of Gaussian errors. \n",
    "\n",
    "If we are varying a single parameter, we can interpert changes in the parameter that result in a 1 unit change of $\\chi^2$ as the $1 \\sigma$ uncertainty on that parameter.  In general, if $\\hat{p}$ is the parameter values that minimize the $\\chi^2$, then we can solve for the uncertainty $\\sigma_p$ using the relationship: \n",
    "\n",
    "$\\Delta \\chi^2 = \\chi^2(\\hat{p} + n \\sigma_p) - \\chi^2(\\hat{p}) = n^2$\n",
    "\n",
    "Where we can pick $n$ to be some convienient number, e.g., 1 in our case.\n",
    "\n",
    "This means that we want to calculate both a curve for $\\chi^2$ as a function of the parameters of interest.  I.e., $\\chi^2(p)$.  \n",
    "\n",
    "(Note that $\\chi^2$ also depends on the data points, and the errors on the data points, but we left that out of the equation to focus on the fact that we are just changing the model parameters and trying to match the data)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "695a8e38",
   "metadata": {},
   "source": [
    "# scanning parameters.\n",
    "\n",
    "We are first going to find the parameter values that minimize the $\\chi^2$ by scanning each of the parameters and computing the resulting $\\chi^2$. \n",
    "\n",
    "First we are going to scan each variable on it's own.  Then we will do a two dimensional scan.\n",
    "\n",
    "We will do $p_0$ (the offset parameter) first.  We will scan from -5 to 5 counts in 11 steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0603bd2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = plt.xlabel(r\"$p_0$ = Offset [counts]\")\n",
    "_ = plt.ylabel(r\"$\\Delta \\chi^2$\")\n",
    "\n",
    "params = np.array([0., 0.])\n",
    "chi2_vals = np.zeros((11))\n",
    "offset_scan_points = np.linspace(-5., 5., 11)\n",
    "for i in range(11):\n",
    "    params[0] = offset_scan_points[i]\n",
    "    chi2_vals[i] = chi2_function(years_since_mid_2014, excess_counts, sigma_counts, linear_function, params)\n",
    "\n",
    "chi2_vals -= chi2_vals.min()\n",
    "_ = plt.plot(offset_scan_points, chi2_vals)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01e4a373",
   "metadata": {},
   "source": [
    "Ok, now let's scan the slope parameter, $p_1$.  We will scan from -2 to 2 counts / year in 11 steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f313f65a",
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = plt.xlabel(r\"$p_{1}$ = Slope [counts/year]\")\n",
    "_ = plt.ylabel(r\"$\\Delta\\chi^2$\")\n",
    "\n",
    "params = np.array([0., 0.])\n",
    "chi2_vals = np.zeros((11))\n",
    "slope_scan_points = np.linspace(-2., 2., 11)\n",
    "for i in range(11):\n",
    "    params[1] = slope_scan_points[i]\n",
    "    chi2_vals[i] = chi2_function(years_since_mid_2014, excess_counts, sigma_counts, linear_function, params)\n",
    "\n",
    "chi2_vals -= chi2_vals.min()\n",
    "_ = plt.plot(slope_scan_points, chi2_vals)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6033a21",
   "metadata": {},
   "source": [
    "# Quick Question\n",
    "\n",
    "7.1 Estimate numerical values for the best-fit values and 1 $\\sigma$ uncertainties on $p_0$ and $p_1$ using the two plots above."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1a151e7",
   "metadata": {},
   "source": [
    "### Extracting multiple parameters at once.\n",
    "\n",
    "Now we are going to extract best-fit values for both $p_0$ and $p_1$ by evaluting the $\\chi^2$ function over points on a two dimensional grid.\n",
    "\n",
    "We plot the result as a color plot, where the color is the $\\Delta \\chi^2$ value, and we are going to draw some contours on the plot correspond to different levels of $\\Delta \\chi^2$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b295c014",
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = plt.xlabel(r\"$p_0$ = Offset [counts]\")\n",
    "_ = plt.ylabel(r\"$p_1$ = Slope [counts/year]\")\n",
    "\n",
    "\n",
    "nx = 51\n",
    "ny = 51\n",
    "\n",
    "params = np.array([0., 0.])\n",
    "chi2_2d_scan_vals = np.zeros((nx, ny))\n",
    "offset_scan_points = np.linspace(-10, 10, nx)\n",
    "slope_scan_points = np.linspace(-5, 5, ny)\n",
    "\n",
    "# Double loop for 2d scan\n",
    "for i in range(nx):\n",
    "    params[0] = offset_scan_points[i]\n",
    "    for j in range(ny):\n",
    "        params[1] = slope_scan_points[j]\n",
    "        chi2_2d_scan_vals[i,j] = chi2_function(years_since_mid_2014, excess_counts, sigma_counts,\n",
    "                                               linear_function, params)\n",
    "\n",
    "min_chi2 = chi2_2d_scan_vals.min()\n",
    "chi2_2d_scan_vals -= min_chi2\n",
    "\n",
    "# This next bit gets the x and y axis values for the grid point at the minimum.\n",
    "idx = chi2_2d_scan_vals.argmin()\n",
    "idx_x = idx//nx\n",
    "idx_y = idx%ny\n",
    "scan_min_x = offset_scan_points[idx_x]\n",
    "scan_min_y = offset_scan_points[idx_y]\n",
    "\n",
    "# Now let's plot it.\n",
    "_ = plt.imshow(chi2_2d_scan_vals.T, extent=(-10, 10, -5, 5), aspect='auto')\n",
    "_ = plt.colorbar(label=r\"$\\Delta \\chi^2$\")\n",
    "_ = plt.contour(offset_scan_points, slope_scan_points, chi2_2d_scan_vals.T, levels=[1, 4, 9], colors=\"white\")\n",
    "_ = plt.scatter(scan_min_x, scan_min_y)\n",
    "print(\"Best fit value is %0.1f for (%0.1f %0.1f)\" % (min_chi2, scan_min_x, scan_min_y))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ae58c08",
   "metadata": {},
   "source": [
    "# Step 6, statistical significance of fit values\n",
    "\n",
    "In the previous step we found a best-fit value of 0.13 +- 0.58 for $p_1$, the slope of the model line.\n",
    "\n",
    "Similarly to what we did last week, we can use `scipy.stats.norm` to compute the p-value.  Once slight difference is that this time we are considering both positive and negative values, so we integrate the Gaussian outward starting from the middle, using the formula in the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a9515db",
   "metadata": {},
   "outputs": [],
   "source": [
    "def two_sided_p_chi2(x, mu, sigma):\n",
    "    return 2*(stats.norm(loc=mu, scale=sigma).sf(np.abs(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16aff7ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "p_val = two_sided_p_chi2(0.13, 0, 0.58)\n",
    "print(\"The p-value to obtain a measurement of %0.2f given a true value of %0.2f and a sigma of %0.2f is %0.2f\" %\n",
    "        (0.13, 0, 0.58, p_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bb12ccb",
   "metadata": {},
   "source": [
    "# Quick question.\n",
    "\n",
    "8.1  Is this result statistically significant?  I.e., should we write a paper saying that Vela is getting dimmer?  Why or why not?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73850ec8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "76966aa2",
   "metadata": {},
   "source": [
    "# Summary of this notebook\n",
    "\n",
    "So, to investiate if the correlation was significant we did a few things:\n",
    "\n",
    "1. A \"model\" with \"parameters\" to describe the data.  In our case we fit the excess as a function of time to a line,  i.e., $m_{\\rm ex}(t | p_0, p_1) = p_0 + t * p_1$ \n",
    "\n",
    "2. A function to compute how close the model is to each data point.   I.e., how well the model fits the data.  To do this we will first compute the residuals for each data point: $\\delta_i = n_{\\rm ex,i} - m_{\\rm ex,i} = n_{\\rm ex,i} - (p_0  + t * p_1)$ \n",
    "\n",
    "3. To interpret the residuals we scaled them by their errors, in this case that is the error bars in the plot above, which are just the square root of the number of observed counts, let's call these $\\sigma_i = \\sqrt{n_{obs, i}}$,  Then we have  $\\chi_i =  \\frac{n_{\\rm ex,i} - m_{\\rm ex,i}}{\\sigma_{obs, i}} = \\frac{n_{\\rm ex,i} - (p_0  + t_i * p_1)}{\\sigma_{obs, i}}$   At that point, if the model were perfect the scaled residuals $\\chi_i$ should each be a variable sampled from a unit Gaussian, i.e., $G(\\chi_i | \\mu=0, \\sigma=1)$.\n",
    "\n",
    "4. The next step was to test how well the model fits the data.  Since we have a bunch of data points that ideally should be represented by Gaussians, we can add the variances in quadrature to get a meaningful quantity.  $\\chi^2 = \\sum_i \\chi_i^2 = \\sum_i (\\frac{n_{\\rm ex,i} - (p_0  + t_i * p_1)}{\\sigma_{obs, i}})^2$.   The $\\chi^2$ tells us how well the model fits the data, on average we expect each data point to contribute about 1 unit to the total $\\chi^2$ (because each point is coming from a unit Gaussian).   Lower $\\chi^2$ represent better fits.\n",
    "\n",
    "5. We varied the input parameters $(p_0, p_1)$ to find the values that minimize the $\\chi^2$.\n",
    "\n",
    "6. Finally, compared the change in $\\chi^2$ between the best fit model and other parameter values and use this to intepret the statistical significance.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5a0e4d1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
